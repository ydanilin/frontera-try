<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>webpilot</title>
  <style>
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <style>
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="devstyle.css" />
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<h1 id="crawling-big-classified-ads-multipaged-portal-with-frontera-and-scrapy">Crawling Big Classified Ads Multipaged Portal with Frontera and Scrapy</h1>
<h2 id="what-is-classified-ads-multipaged-portal">What is Classified Ads Multipaged Portal</h2>
<p>Sometimes customers ask to extract data from websites, stuffed with tons of classifieds like companies advertisments, job offerings, goods listings etc. We call such websites Classified Ads Multipaged Portal and will name it here just <strong>portal</strong> for simplicity.</p>
<p>Often such portals have a <strong>search</strong> box in the main page, which leads you to <strong>listing pages</strong> when you enter a search criteria. Every classified ad on the listing (which we will refer as <strong>item</strong>) contains mostly brief information like name, address, web, which we need to scrape. Also, the customer may ask you to grab additional details from item <strong>details page</strong>, which you can get when you click on a classified ad.</p>
<p>The customer gives you about dozen or two of search criterias which we inject into our scraping system as initial (<strong>seed</strong>) URLs (via a text or CSV file) and the process begins.</p>
<p>Below is a typical structure and workflow to do a job:</p>
<figure>
<img src="webpilot/camp_elems.svg" alt="Elements of a Classified Ads Multipaged Portal" class="center" /><figcaption>Elements of a Classified Ads Multipaged Portal</figcaption>
</figure>
<h2 id="problem-statement-why-do-we-need-a-job-state-persistency-in-our-scraper">Problem Statement: Why do we need a “Job State Persistency” in our Scraper</h2>
<p>If we’re talking about scraping a portal, which contains hundreds of thousands records, <strong>it is unlikely that we will do all our job in a single session</strong>. Several reasons may be for that – perhaps we want to inspect the portion of the data collected before proceeding further (which is definitely a good practice!) or we know that, if we exceed some amount of requests the site will temporary ban us so we need to stop and recharge the session after waiting some time, etc.</p>
<p>Furthermore, we must be prepared to face with errors and session breakdowns on whatever step. Never can we expect that data or webpages are always uniform and therefore we may catch an error, which we could not ever imagine.</p>
<p>That is why we need to maintain persistency in our scraper - to be able to start exactly from the point where we finished before. This way we need the mechanisms to handle the following use cases:</p>
<ul>
<li>skip already processed searches</li>
<li>skip already processed pages within firm listing of a category</li>
<li>(optional) avoid duplicates in scraped items</li>
</ul>
<p>After some experience with “handmade” solutions, we considered to use <a href="https://scrapinghub.com/open-source">Scrapinghub’s open source</a> <a href="https://github.com/scrapinghub/frontera">Frontera</a> package, which does exactly what we need: keeps the scraping job state persistent in a database so we can keep calm that scraper will restart exactly from where it stopped and nothing will be missed.</p>
<h2 id="able-to-see-job-progress-discovery-crawls-and-payload-crawls">Able to see Job Progress: Discovery Crawls and Payload Crawls</h2>
<p>Scraping a huge portal may take hours and even days. Obviously it is a good idea to see where we are and, more important, to see when we finish. To make such “progress indicator”, we must know how much work we have in total. Coming back to our multipaged portal, this “work estimation” process can be expressed as <strong>discovery crawl</strong> to do the following:</p>
<ul>
<li>walk only across paging links first</li>
<li>record amount of pages when we reached the last page within each search</li>
<li>record amount of items on each page - to signal when a particular page is completed during <strong>payload crawl</strong> and to calculate overall total</li>
</ul>
<p>Next two items are not related directly to discovery process but should be implemented at discovery crawl stage in order not to do same work twice:</p>
<ul>
<li>collect links to items details pages</li>
<li>scrape information about items which is already available on listing pages</li>
</ul>
<p>Due to it’s built-in mechanism of <strong>link scoring</strong>, <a href="https://github.com/scrapinghub/frontera">Frontera</a> package is able to handle such two-phase crawl mechanism with just small alterations in code.</p>
<p>Last, to illustrate discussed above, we provide an idea how scraper monitoring dashboard may look like:</p>
<figure>
<img src="webpilot/discover_dash.svg" alt="Dashboard in Discovery mode" class="center" /><figcaption>Dashboard in Discovery mode</figcaption>
</figure>
<figure>
<img src="webpilot/payload_dash.svg" alt="Dashboard in Payload mode" class="center" /><figcaption>Dashboard in Payload mode</figcaption>
</figure>
<h2 id="frontera-single-process-workflow">Frontera Single Process Workflow</h2>
<p>Scraping job using Frontera starts with injecting into the system an initial requests, which are called <strong>seeds</strong>. This injection is performed via separate small script, which saves the seeds directly into the database (Queue table).</p>
<p>After that you turn the key and the system starts. The drive unit in the circle is Scrapy Engine, which periodically asks Frontera Scheduler for new requests. Then Scrapy fires the requests into Internet, extract new links from responses, convert these links into new Requests and push them back to the Scheduler. Now this is Frontera’s turn.</p>
<p>The goal of crawling frontier is to save, arrange, order and deliver new requests to spiders. The scheme below introduces the Frontera functionality. From this scheme we will see where to insert our customized functionality - Job State Persistency and Job Progress Reporting.</p>
<figure>
<img src="webpilot/frontera_workflow.svg" alt="Frontera Single Process Workflow" class="center" /><figcaption>Frontera Single Process Workflow</figcaption>
</figure>
<h2 id="webpilot-protocol">WebPilot protocol</h2>
<p>We mentioned above that Scrapy extracts new links form pages he visited. On the ads portal workflow scheme we see that, on Listing Pages there are two types of links: Paging link (B) and Item details links (C). We need to identify each link precisely: what kind of link it is, where it originated from and what it targets to.</p>
<p>The situation is that links actually originating in the Scrapy side, but will be handled on the Frontera side. Therefore they must communicate the link details and understand each other. We must agree on data formats and we propose to establish this as <strong>WebPilot protocol</strong>.</p>
<ol type="1">
<li>Both Scrapy and Frontera have (by design) special field called <code>meta</code> to store arbitrary information in the <code>Request</code> and <code>Response</code> classes.</li>
<li>Block of information about link is called <code>flight</code> and to be stored in meta under <code>meta['flight']</code> key.</li>
<li><code>meta['flight']</code> data structure must be carried out through the whole (single) Request-Response-Request-Response… chain. The fact is that the frameworks copy meta field form <code>Request</code> to <code>Response</code> automatically, but <strong>do not do so</strong> for <code>Request</code>s, <strong>newly born from extracting links</strong> from <code>Response</code> webpage.</li>
<li>To describe <code>flight</code> data structure, we show the scraping process from Fig. 1 in simplified form but showing the <code>meta['flight']</code> examples.</li>
</ol>
<figure>
<img src="webpilot/flight_struct.svg" alt="“Flight” data structure usage" class="center" /><figcaption>“Flight” data structure usage</figcaption>
</figure>
<h3 id="scrapy-setup">Scrapy setup</h3>
<p>Everyone should do their own job and do it well. Scrapy should only download pages, process them for links and scrape data items. Frontera should manage the process. Therefore it would be nice to concentrate all WepPilot flight record management inside Frontera, however some minimal setup required for Scrapy.</p>
<p>Everybody familiar with Scrapy knows that there is convenient <code>CrawlSpider</code> sublclass, which introduces Link Extractor Rules thus reduces the amount of code. At the time of this writing Frontera does not support CrawlSpiders but we can mimic its functionality in a rather declarative way.</p>
<p>First, we declare two <a href="https://docs.scrapy.org/en/latest/topics/link-extractors.html">LinkExtractor</a>s in spider’s <code>__init__</code>:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode numberSource python numberLines"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" title="1"><span class="im">from</span> scrapy.linkextractors <span class="im">import</span> LinkExtractor</a>
<a class="sourceLine" id="cb1-2" title="2"></a>
<a class="sourceLine" id="cb1-3" title="3"><span class="co"># spider class</span></a>
<a class="sourceLine" id="cb1-4" title="4">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, <span class="op">*</span>args, <span class="op">**</span>kwargs):</a>
<a class="sourceLine" id="cb1-5" title="5">        <span class="bu">super</span>(OurSpider, <span class="va">self</span>).<span class="fu">__init__</span>(<span class="op">*</span>args, <span class="op">**</span>kwargs)</a>
<a class="sourceLine" id="cb1-6" title="6">        <span class="va">self</span>.le_paging <span class="op">=</span> LinkExtractor(</a>
<a class="sourceLine" id="cb1-7" title="7">            <span class="co"># rules for PAGING link as per Scrapy docs</span></a>
<a class="sourceLine" id="cb1-8" title="8">        )</a>
<a class="sourceLine" id="cb1-9" title="9">        <span class="va">self</span>.le_items <span class="op">=</span> LinkExtractor(</a>
<a class="sourceLine" id="cb1-10" title="10">            <span class="co"># rules for ITEM DETAILS links as per Scrapy docs</span></a>
<a class="sourceLine" id="cb1-11" title="11">        )</a></code></pre></div>
<p>When Scrapy has downloaded a page, it invokes (by default) <code>parse()</code> callback to process the response. This is the place where our <code>LinkExtractor</code>s come into play. Remember that paging links are of the first priority, so first we call <code>extract_links()</code> from the paging extractor, than from items one.</p>
<p>Essentially <code>parse()</code> is a generator, which must yield <code>Request</code> instances. So we process link in each list and yield from chain of the two iterators:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode numberSource python numberLines"><code class="sourceCode python"><a class="sourceLine" id="cb2-1" title="1"><span class="im">from</span> itertools <span class="im">import</span> chain</a>
<a class="sourceLine" id="cb2-2" title="2"></a>
<a class="sourceLine" id="cb2-3" title="3">    <span class="kw">def</span> parse(<span class="va">self</span>, response):</a>
<a class="sourceLine" id="cb2-4" title="4">        <span class="cf">if</span> <span class="kw">not</span> <span class="bu">isinstance</span>(response, HtmlResponse):</a>
<a class="sourceLine" id="cb2-5" title="5">            <span class="cf">return</span></a>
<a class="sourceLine" id="cb2-6" title="6">        <span class="co"># find paging links</span></a>
<a class="sourceLine" id="cb2-7" title="7">        paging_links <span class="op">=</span> <span class="va">self</span>.le_paging.extract_links(response)</a>
<a class="sourceLine" id="cb2-8" title="8">        paging_requests <span class="op">=</span> <span class="bu">map</span>(<span class="va">self</span>.make_paging_request, paging_links)</a>
<a class="sourceLine" id="cb2-9" title="9">        <span class="co"># find items</span></a>
<a class="sourceLine" id="cb2-10" title="10">        items_links <span class="op">=</span> <span class="va">self</span>.le_items.extract_links(response)</a>
<a class="sourceLine" id="cb2-11" title="11">        items_requests <span class="op">=</span> <span class="bu">map</span>(<span class="va">self</span>.make_item_request, items_links)</a>
<a class="sourceLine" id="cb2-12" title="12">        </a>
<a class="sourceLine" id="cb2-13" title="13">        <span class="cf">yield</span> <span class="im">from</span> chain(paging_requests, items_requests)</a></code></pre></div>
<p><strong>Now the most important part of this chapter</strong>: we could simply do <code>lambda link: Request(link.url)</code> directly in <code>map</code>s, why we create separate methods <code>make_paging_request()</code> and <code>make_item_request()</code>? The reason is to update <code>meta</code> keywords according to our protocol conventions and diagram above!</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode numberSource python numberLines"><code class="sourceCode python"><a class="sourceLine" id="cb3-1" title="1">    <span class="at">@staticmethod</span></a>
<a class="sourceLine" id="cb3-2" title="2">    <span class="kw">def</span> make_paging_request(link):</a>
<a class="sourceLine" id="cb3-3" title="3">        url, page_to <span class="op">=</span> extract_page_number_somehow(link)  <span class="co"># implementation depends</span></a>
<a class="sourceLine" id="cb3-4" title="4">        <span class="co"># flight structure points to page, whose number saved in &#39;to&#39; field</span></a>
<a class="sourceLine" id="cb3-5" title="5">        flight <span class="op">=</span> {</a>
<a class="sourceLine" id="cb3-6" title="6">            <span class="st">&#39;dest&#39;</span>: <span class="st">&#39;page&#39;</span>,</a>
<a class="sourceLine" id="cb3-7" title="7">            <span class="st">&#39;to&#39;</span>: page_to,</a>
<a class="sourceLine" id="cb3-8" title="8">        }</a>
<a class="sourceLine" id="cb3-9" title="9">        r <span class="op">=</span> Request(url)</a>
<a class="sourceLine" id="cb3-10" title="10">        <span class="co"># use only update() to safely append/modify</span></a>
<a class="sourceLine" id="cb3-11" title="11">        r.meta.update(flight<span class="op">=</span>flight)</a>
<a class="sourceLine" id="cb3-12" title="12">        <span class="cf">return</span> r</a>
<a class="sourceLine" id="cb3-13" title="13"></a>
<a class="sourceLine" id="cb3-14" title="14">    <span class="at">@staticmethod</span></a>
<a class="sourceLine" id="cb3-15" title="15">    <span class="kw">def</span> make_item_request(link):</a>
<a class="sourceLine" id="cb3-16" title="16">        <span class="co"># suppose we need firma id for database purpose</span></a>
<a class="sourceLine" id="cb3-17" title="17">        firma_id <span class="op">=</span> get_id_from_path(url)</a>
<a class="sourceLine" id="cb3-18" title="18">        <span class="co"># this time link points to item details page</span></a>
<a class="sourceLine" id="cb3-19" title="19">        flight <span class="op">=</span> {</a>
<a class="sourceLine" id="cb3-20" title="20">            <span class="st">&#39;dest&#39;</span>: <span class="st">&#39;item&#39;</span>,</a>
<a class="sourceLine" id="cb3-21" title="21">            <span class="st">&#39;payload&#39;</span>: {<span class="st">&#39;firma_id&#39;</span>: firma_id}  <span class="co"># remember the &#39;payload&#39; field</span></a>
<a class="sourceLine" id="cb3-22" title="22">                                               <span class="co"># from the diagram above</span></a>
<a class="sourceLine" id="cb3-23" title="23">        }</a>
<a class="sourceLine" id="cb3-24" title="24">        r <span class="op">=</span> Request(url)</a>
<a class="sourceLine" id="cb3-25" title="25">        r.meta.update(flight<span class="op">=</span>flight)</a>
<a class="sourceLine" id="cb3-26" title="26">        <span class="cf">return</span> r</a></code></pre></div>
<p>To summarize this part: only Scrapy knows if he got paging link or item details link, and only Scrapy can extract the page number paging link points to. Therefore we need this bare minimum setup to correctly adjust <code>flight</code> information for further Frontera processing.</p>
<h3 id="frontera-setup">Frontera setup</h3>
<h4 id="how-things-are-started">How things are started</h4>
<p>As you see from the diagram above, scraping job is a round-trip process of feeding requests - receiving responses - generation of next requests. The question remains how to inject initial requests befor turning the key to start the engine. To switch on “onboard power supply” of the machine Frontera provides special utility script <code>python -m frontera.utils.add_seeds --config &lt;path.to.frontera.settings&gt; --seeds-file seeds.txt</code>, which does two things:</p>
<ul>
<li>create the database with all necessary tables</li>
<li>opens up a text file with seed URLs</li>
<li>calls <code>read_seeds()</code> method of <code>Strategy</code> class, with file <code>stream</code> argument</li>
</ul>
<p>We mention here that in this text file we can supply custom information, along with seed URLs. Suppose customer ask us to track from which search criteria items were originated. This way feed txt file can be orgainized like csv as follows</p>
<pre><code>url, search
https://www.example.com/list?query=search%2001, Search 01
https://www.example.com/list?query=search%2002, Search 02</code></pre>
<p>Now we need to override <code>read_seeds()</code> method to adapt it to read our csv structure:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode numberSource python numberLines"><code class="sourceCode python"><a class="sourceLine" id="cb5-1" title="1"><span class="im">from</span> io <span class="im">import</span> TextIOWrapper</a>
<a class="sourceLine" id="cb5-2" title="2"></a>
<a class="sourceLine" id="cb5-3" title="3">    <span class="kw">def</span> read_seeds(<span class="va">self</span>, stream):</a>
<a class="sourceLine" id="cb5-4" title="4">        text <span class="op">=</span> TextIOWrapper(stream, encoding<span class="op">=</span><span class="st">&#39;utf-8&#39;</span>)</a>
<a class="sourceLine" id="cb5-5" title="5">        csv_records <span class="op">=</span> DictReader(text, skipinitialspace<span class="op">=</span><span class="va">True</span>)</a>
<a class="sourceLine" id="cb5-6" title="6">        <span class="cf">for</span> entry <span class="kw">in</span> csv_records:</a>
<a class="sourceLine" id="cb5-7" title="7">            url <span class="op">=</span> entry.pop(<span class="st">&#39;url&#39;</span>)</a>
<a class="sourceLine" id="cb5-8" title="8">            flight <span class="op">=</span> {</a>
<a class="sourceLine" id="cb5-9" title="9">                <span class="st">&#39;dest&#39;</span>: <span class="st">&#39;page&#39;</span>,</a>
<a class="sourceLine" id="cb5-10" title="10">                <span class="st">&#39;from&#39;</span>: <span class="st">&#39;0&#39;</span>,</a>
<a class="sourceLine" id="cb5-11" title="11">                <span class="st">&#39;to&#39;</span>: <span class="st">&#39;1&#39;</span>,</a>
<a class="sourceLine" id="cb5-12" title="12">                <span class="st">&#39;details&#39;</span>: <span class="bu">dict</span>(entry),</a>
<a class="sourceLine" id="cb5-13" title="13">            }</a>
<a class="sourceLine" id="cb5-14" title="14">            r <span class="op">=</span> <span class="va">self</span>.create_request(url.strip(), meta<span class="op">=</span><span class="bu">dict</span>(flight<span class="op">=</span>flight))</a>
<a class="sourceLine" id="cb5-15" title="15">            <span class="va">self</span>.schedule(r)</a></code></pre></div>
<p>Here <code>TextIOWrapper</code> is needed because Frontera supplies <code>stream</code> argument as binary stream. Creation of <code>flight</code> structure in <em>start</em> requests was discussed in case A1 of “Flight data structure usage” figure, the only trick here is, that after we popped <code>'url'</code> key from <code>DictReader</code> entry, the rest of that disctionary may contain <em>arbitrary amount of custom data</em>, not only “search” as in our example.</p>
<p>The last action <code>read_seeds()</code> must perform is to call <code>schedule()</code> method of its <code>Strategy</code> class.</p>
<p><strong>Note:</strong> we would not discuss here how to store custom data in The Database, that’s rather easy with Frontera framework but we would like to shift this discussion into separate section. Just mention here shortly that we would need to override <code>Queue.schedule()</code> method to include custom field in <code>QueueModel</code> class instantiation.</p>
<h4 id="webpilot-metaflight-data-structure-handover-to-newly-discovered-links">WebPilot <code>meta['flight']</code> data structure handover to newly discovered links</h4>
<p>In the introduction to WebPilot protocol (Item 4) we mentioned that the freameworks do not pass <code>meta</code> information from origin <code>Request</code> to <code>Request</code>s spawned by it. Luckily, this can be implemented within the same project <code>Strategy</code> class. Our method of interest is <code>links_extracted(request, links)</code>, where parameters are:</p>
<ul>
<li><code>request</code>: the original request, which fetched webpage from which LinkExtractors scraped new links;</li>
<li><code>links</code>: the list of <code>Request</code>s spawned by the original request. We guess name “links” creates a little bit misunderstanding here, because these entities in fact are not URLs but already <code>Request</code> instances, pointing to these URLs.</li>
</ul>
</body>
</html>
